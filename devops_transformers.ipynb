{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from datasets import load_metric\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mricardmask\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=transformers_devops\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=transformers_devops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_IND2LABEL = {\n",
    "    0: 'Computer Science',\n",
    "    1: 'Economics',\n",
    "    2: 'Electrical Engineering and Systems Science',\n",
    "    3: 'Mathematics',\n",
    "    4: 'Physics',\n",
    "    5: 'Quantitative Biology',\n",
    "    6: 'Quantitative Finance',\n",
    "    7: 'Statistics',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data['text'].tolist(),\n",
    "                                                  data.target.values,\n",
    "                                                  test_size=0.17, stratify=data.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt')\n",
    "val_texts = tokenizer(X_val, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ArxivDataset(train_texts, y_train)\n",
    "val_dataset = ArxivDataset(val_texts, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super().__init__()\n",
    "        self.encoder = DistilBertModel.from_pretrained(\"distilbert-base-cased\")\n",
    "        self.pre_classifier = nn.Linear(768, 768)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.dropout(self.gelu(self.pre_classifier(pooler)))\n",
    "        preds = self.classifier(pooler)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBERTClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = load_metric(\"accuracy\")\n",
    "    load_f1 = load_metric(\"f1\")\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    length = min(len(labels), len(logits))\n",
    "    logits = logits[:length]\n",
    "    labels = labels[:length]\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    accuracy = load_accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1 = load_f1.compute(predictions=preds, references=labels, average='macro')[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir = \"./results_dbb\",\n",
    "    report_to = \"wandb\",\n",
    "    run_name = 'base_distilbert_run_2',\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 130,\n",
    "    logging_dir = './logs_dbb',\n",
    "    logging_steps = 130,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    weight_decay = 0.01,\n",
    "    max_steps = 6500,\n",
    "    warmup_steps = 500,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get('labels')\n",
    "        outputs = model(**inputs)\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(outputs, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(\n",
    "    model = model,\n",
    "    args = train_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebook/ods_alpha/Arxiv/wandb/run-20230417_074447-49c17kvm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ricardmask/transformers_devops/runs/49c17kvm' target=\"_blank\">base_distilbert_run_2</a></strong> to <a href='https://wandb.ai/ricardmask/transformers_devops' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ricardmask/transformers_devops' target=\"_blank\">https://wandb.ai/ricardmask/transformers_devops</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ricardmask/transformers_devops/runs/49c17kvm' target=\"_blank\">https://wandb.ai/ricardmask/transformers_devops/runs/49c17kvm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6500' max='6500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6500/6500 2:41:42, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.398900</td>\n",
       "      <td>0.826184</td>\n",
       "      <td>0.576250</td>\n",
       "      <td>0.128489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.709333</td>\n",
       "      <td>0.546331</td>\n",
       "      <td>0.138303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.720600</td>\n",
       "      <td>0.673016</td>\n",
       "      <td>0.521630</td>\n",
       "      <td>0.140675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.670369</td>\n",
       "      <td>0.490390</td>\n",
       "      <td>0.141845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.680600</td>\n",
       "      <td>0.671164</td>\n",
       "      <td>0.480880</td>\n",
       "      <td>0.139944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.661700</td>\n",
       "      <td>0.634803</td>\n",
       "      <td>0.503533</td>\n",
       "      <td>0.141313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.655900</td>\n",
       "      <td>0.616959</td>\n",
       "      <td>0.523281</td>\n",
       "      <td>0.136932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>0.616798</td>\n",
       "      <td>0.520111</td>\n",
       "      <td>0.140248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.617500</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>0.485965</td>\n",
       "      <td>0.138582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.623200</td>\n",
       "      <td>0.632411</td>\n",
       "      <td>0.534971</td>\n",
       "      <td>0.135427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.626900</td>\n",
       "      <td>0.607860</td>\n",
       "      <td>0.534311</td>\n",
       "      <td>0.135666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.586900</td>\n",
       "      <td>0.600027</td>\n",
       "      <td>0.501024</td>\n",
       "      <td>0.140613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.615100</td>\n",
       "      <td>0.610182</td>\n",
       "      <td>0.481474</td>\n",
       "      <td>0.138905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.591406</td>\n",
       "      <td>0.497655</td>\n",
       "      <td>0.139695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.585900</td>\n",
       "      <td>0.592243</td>\n",
       "      <td>0.511789</td>\n",
       "      <td>0.137697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.591900</td>\n",
       "      <td>0.593173</td>\n",
       "      <td>0.510931</td>\n",
       "      <td>0.139703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.567900</td>\n",
       "      <td>0.589929</td>\n",
       "      <td>0.493230</td>\n",
       "      <td>0.139444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.585200</td>\n",
       "      <td>0.588179</td>\n",
       "      <td>0.494419</td>\n",
       "      <td>0.139813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>0.507300</td>\n",
       "      <td>0.583274</td>\n",
       "      <td>0.506968</td>\n",
       "      <td>0.138612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.521800</td>\n",
       "      <td>0.580756</td>\n",
       "      <td>0.502939</td>\n",
       "      <td>0.140515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>0.523100</td>\n",
       "      <td>0.585339</td>\n",
       "      <td>0.522489</td>\n",
       "      <td>0.138068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.511300</td>\n",
       "      <td>0.591882</td>\n",
       "      <td>0.512846</td>\n",
       "      <td>0.141167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>0.588031</td>\n",
       "      <td>0.492768</td>\n",
       "      <td>0.141091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.516300</td>\n",
       "      <td>0.578244</td>\n",
       "      <td>0.502080</td>\n",
       "      <td>0.140349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.599956</td>\n",
       "      <td>0.494155</td>\n",
       "      <td>0.139399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>0.513400</td>\n",
       "      <td>0.582705</td>\n",
       "      <td>0.524734</td>\n",
       "      <td>0.134724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3510</td>\n",
       "      <td>0.526200</td>\n",
       "      <td>0.578218</td>\n",
       "      <td>0.515620</td>\n",
       "      <td>0.139426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.583079</td>\n",
       "      <td>0.525791</td>\n",
       "      <td>0.136787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3770</td>\n",
       "      <td>0.515500</td>\n",
       "      <td>0.575211</td>\n",
       "      <td>0.498910</td>\n",
       "      <td>0.138675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.508900</td>\n",
       "      <td>0.584990</td>\n",
       "      <td>0.509478</td>\n",
       "      <td>0.135778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4030</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.570879</td>\n",
       "      <td>0.518328</td>\n",
       "      <td>0.136039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>0.495100</td>\n",
       "      <td>0.584891</td>\n",
       "      <td>0.518064</td>\n",
       "      <td>0.137019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4290</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.577526</td>\n",
       "      <td>0.502080</td>\n",
       "      <td>0.138358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>0.524100</td>\n",
       "      <td>0.576007</td>\n",
       "      <td>0.480021</td>\n",
       "      <td>0.140277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>0.574828</td>\n",
       "      <td>0.506836</td>\n",
       "      <td>0.138735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>0.517600</td>\n",
       "      <td>0.563142</td>\n",
       "      <td>0.507760</td>\n",
       "      <td>0.138914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4810</td>\n",
       "      <td>0.454900</td>\n",
       "      <td>0.595806</td>\n",
       "      <td>0.491711</td>\n",
       "      <td>0.140718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4940</td>\n",
       "      <td>0.421300</td>\n",
       "      <td>0.611370</td>\n",
       "      <td>0.461264</td>\n",
       "      <td>0.141489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5070</td>\n",
       "      <td>0.434600</td>\n",
       "      <td>0.604623</td>\n",
       "      <td>0.496533</td>\n",
       "      <td>0.140152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.601265</td>\n",
       "      <td>0.494815</td>\n",
       "      <td>0.139530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5330</td>\n",
       "      <td>0.427300</td>\n",
       "      <td>0.598043</td>\n",
       "      <td>0.491711</td>\n",
       "      <td>0.140589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5460</td>\n",
       "      <td>0.400500</td>\n",
       "      <td>0.598617</td>\n",
       "      <td>0.498514</td>\n",
       "      <td>0.138242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5590</td>\n",
       "      <td>0.416700</td>\n",
       "      <td>0.596005</td>\n",
       "      <td>0.502345</td>\n",
       "      <td>0.139885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5720</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.609018</td>\n",
       "      <td>0.496268</td>\n",
       "      <td>0.140167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.609578</td>\n",
       "      <td>0.484776</td>\n",
       "      <td>0.140518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5980</td>\n",
       "      <td>0.417100</td>\n",
       "      <td>0.604468</td>\n",
       "      <td>0.481804</td>\n",
       "      <td>0.141155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6110</td>\n",
       "      <td>0.430400</td>\n",
       "      <td>0.604631</td>\n",
       "      <td>0.484975</td>\n",
       "      <td>0.140659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6240</td>\n",
       "      <td>0.409200</td>\n",
       "      <td>0.605291</td>\n",
       "      <td>0.494815</td>\n",
       "      <td>0.140350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6370</td>\n",
       "      <td>0.401200</td>\n",
       "      <td>0.606469</td>\n",
       "      <td>0.502146</td>\n",
       "      <td>0.139924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.604430</td>\n",
       "      <td>0.498778</td>\n",
       "      <td>0.140722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8276fc8c9ee429d8968cee063e83a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.678194…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>█▆▅▃▄▅▅▃▅▃▂▃▄▃▃▄▅▄▃▃▅▄▅▃▄▄▃▂▄▃▁▃▃▃▄▃▂▂▃▃</td></tr><tr><td>eval/f1</td><td>▁▆▇██▅▇▆▅▇▆▇▇▇▇▆▆██▇▄▇▅▆▅▅▆▇▆▇█▇▇▆▇▇█▇▇▇</td></tr><tr><td>eval/loss</td><td>█▅▄▄▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>eval/runtime</td><td>▃▃▄▃▃▄▃▄▄▄▄▂▃▃▃▄▄▄█▂▂▅▂▂▂▂▃▂▁▁▂▃▃▃▃▂▁▂▂▆</td></tr><tr><td>eval/samples_per_second</td><td>▆▆▅▆▆▅▆▅▅▅▅▇▆▅▆▅▅▅▁▇▇▄▇▇▆▇▆▇██▆▆▆▆▆▆█▆▇▃</td></tr><tr><td>eval/steps_per_second</td><td>▆▆▅▆▆▅▆▅▅▅▅▇▆▅▆▅▅▅▁▇▇▄▇▇▆▇▆▇██▆▆▆▆▆▇█▆▇▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▃▅▆███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.49878</td></tr><tr><td>eval/f1</td><td>0.14072</td></tr><tr><td>eval/loss</td><td>0.60443</td></tr><tr><td>eval/runtime</td><td>106.7336</td></tr><tr><td>eval/samples_per_second</td><td>144.116</td></tr><tr><td>eval/steps_per_second</td><td>2.258</td></tr><tr><td>train/epoch</td><td>2.77</td></tr><tr><td>train/global_step</td><td>6500</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4115</td></tr><tr><td>train/total_flos</td><td>0.0</td></tr><tr><td>train/train_loss</td><td>0.54459</td></tr><tr><td>train/train_runtime</td><td>9718.2966</td></tr><tr><td>train/train_samples_per_second</td><td>21.403</td></tr><tr><td>train/train_steps_per_second</td><td>0.669</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">base_distilbert_run_2</strong> at: <a href='https://wandb.ai/ricardmask/transformers_devops/runs/49c17kvm' target=\"_blank\">https://wandb.ai/ricardmask/transformers_devops/runs/49c17kvm</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230417_074447-49c17kvm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
